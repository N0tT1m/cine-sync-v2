{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering Tutorial\n",
    "\n",
    "This notebook demonstrates how to train and use Neural Collaborative Filtering models for movie recommendations.\n",
    "\n",
    "## Overview\n",
    "- **Neural Collaborative Filtering (NCF)**: Deep learning approach combining GMF and MLP\n",
    "- **Dataset**: MovieLens 32M with 32M+ ratings from 280K users\n",
    "- **Goal**: Learn complex user-item interaction patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install torch pandas scikit-learn numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model import NeuralCollaborativeFiltering, SimpleNCF\n",
    "from data_loader import NCFDataLoader\n",
    "from trainer import NCFTrainer\n",
    "from inference import NCFInference\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MovieLens data (adjust paths as needed)\n",
    "ratings_path = '../../ml-32m/ratings.csv'\n",
    "movies_path = '../../ml-32m/movies.csv'\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(ratings_path):\n",
    "    print(f\"Please download MovieLens 32M dataset and place ratings.csv at {ratings_path}\")\n",
    "    print(\"Download from: https://files.grouplens.org/datasets/movielens/ml-32m.zip\")\n",
    "else:\n",
    "    print(f\"Found ratings file: {ratings_path}\")\n",
    "    \n",
    "# Load data\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "movies_df = pd.read_csv(movies_path) if os.path.exists(movies_path) else None\n",
    "\n",
    "print(f\"Ratings shape: {ratings_df.shape}\")\n",
    "print(f\"Movies shape: {movies_df.shape if movies_df is not None else 'Not loaded'}\")\n",
    "\n",
    "# Display sample data\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total ratings: {len(ratings_df):,}\")\n",
    "print(f\"Unique users: {ratings_df['userId'].nunique():,}\")\n",
    "print(f\"Unique movies: {ratings_df['movieId'].nunique():,}\")\n",
    "print(f\"Rating range: {ratings_df['rating'].min()} - {ratings_df['rating'].max()}\")\n",
    "print(f\"Average rating: {ratings_df['rating'].mean():.2f}\")\n",
    "\n",
    "# Plot rating distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "ratings_df['rating'].hist(bins=20, alpha=0.7)\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "user_counts = ratings_df['userId'].value_counts()\n",
    "plt.hist(user_counts, bins=50, alpha=0.7)\n",
    "plt.title('User Activity Distribution')\n",
    "plt.xlabel('Number of Ratings per User')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "movie_counts = ratings_df['movieId'].value_counts()\n",
    "plt.hist(movie_counts, bins=50, alpha=0.7)\n",
    "plt.title('Movie Popularity Distribution')\n",
    "plt.xlabel('Number of Ratings per Movie')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader with preprocessing\n",
    "data_loader = NCFDataLoader(\n",
    "    ratings_path=ratings_path,\n",
    "    movies_path=movies_path,\n",
    "    min_ratings_per_user=20,  # Filter users with at least 20 ratings\n",
    "    min_ratings_per_item=20   # Filter movies with at least 20 ratings\n",
    ")\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Filtered to {len(data_loader.ratings_df)} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test splits\n",
    "train_loader, val_loader, test_loader = data_loader.get_data_loaders(\n",
    "    batch_size=1024,\n",
    "    num_workers=2  # Adjust based on your system\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Get model configuration\n",
    "model_config = data_loader.get_model_config()\n",
    "print(f\"\\nModel configuration: {model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NCF model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Start with SimpleNCF for faster training\n",
    "model = SimpleNCF(\n",
    "    num_users=model_config['num_users'],\n",
    "    num_items=model_config['num_items'],\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = NCFTrainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Train model (adjust epochs based on your time constraints)\n",
    "print(\"Starting training...\")\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=10,  # Increase for better performance\n",
    "    patience=5,\n",
    "    save_dir='../models'\n",
    ")\n",
    "\n",
    "print(f\"Training completed! Best validation loss: {trainer.best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['train_losses'], label='Train Loss')\n",
    "plt.plot(history['val_losses'], label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['val_rmses'], label='RMSE', color='orange')\n",
    "plt.title('Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history['val_maes'], label='MAE', color='green')\n",
    "plt.title('Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_metrics = trainer.evaluate(test_loader)\n",
    "\n",
    "print(\"Test Set Results:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model and encoders\n",
    "data_loader.save_encoders('../models/encoders.pkl')\n",
    "print(\"\\nModel and encoders saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference object\n",
    "inference = NCFInference(\n",
    "    model_path='../models/best_model.pt',\n",
    "    encoders_path='../models/encoders.pkl',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Inference object created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample user for demonstration\n",
    "sample_user = data_loader.ratings_df['userId'].iloc[0]\n",
    "print(f\"Getting recommendations for user {sample_user}\")\n",
    "\n",
    "# Get user's rating history\n",
    "user_history = data_loader.ratings_df[data_loader.ratings_df['userId'] == sample_user]\n",
    "print(f\"User has {len(user_history)} ratings\")\n",
    "\n",
    "# Show some of user's highly rated movies\n",
    "high_rated = user_history[user_history['rating'] >= 4.0].sort_values('rating', ascending=False)\n",
    "if movies_df is not None:\n",
    "    high_rated_with_titles = high_rated.merge(movies_df, on='movieId')\n",
    "    print(\"\\nUser's highly rated movies:\")\n",
    "    for _, row in high_rated_with_titles.head(5).iterrows():\n",
    "        print(f\"  {row['title']}: {row['rating']}/5.0\")\n",
    "\n",
    "# Get seen items to exclude from recommendations\n",
    "seen_items = user_history['movieId'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations\n",
    "recommendations = inference.get_user_recommendations(\n",
    "    user_id=sample_user,\n",
    "    top_k=10,\n",
    "    exclude_seen=True,\n",
    "    seen_items=seen_items\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 10 recommendations for user {sample_user}:\")\n",
    "for i, (movie_id, score) in enumerate(recommendations, 1):\n",
    "    if movies_df is not None:\n",
    "        movie_info = movies_df[movies_df['movieId'] == movie_id]\n",
    "        if not movie_info.empty:\n",
    "            title = movie_info.iloc[0]['title']\n",
    "            genres = movie_info.iloc[0]['genres']\n",
    "            print(f\"  {i}. {title} (Score: {score:.3f})\")\n",
    "            print(f\"     Genres: {genres}\")\n",
    "        else:\n",
    "            print(f\"  {i}. Movie ID {movie_id} (Score: {score:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {i}. Movie ID {movie_id} (Score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rating prediction\n",
    "test_pairs = [(sample_user, rec[0]) for rec in recommendations[:3]]\n",
    "\n",
    "print(f\"\\nRating predictions for user {sample_user}:\")\n",
    "for user_id, movie_id in test_pairs:\n",
    "    predicted_rating = inference.predict_rating(user_id, movie_id)\n",
    "    if movies_df is not None:\n",
    "        movie_info = movies_df[movies_df['movieId'] == movie_id]\n",
    "        title = movie_info.iloc[0]['title'] if not movie_info.empty else f\"Movie {movie_id}\"\n",
    "        print(f\"  {title}: {predicted_rating:.2f}/5.0\")\n",
    "    else:\n",
    "        print(f\"  Movie {movie_id}: {predicted_rating:.2f}/5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a popular movie - find similar items\n",
    "if movies_df is not None:\n",
    "    # Find a popular movie\n",
    "    movie_popularity = data_loader.ratings_df['movieId'].value_counts()\n",
    "    popular_movie = movie_popularity.index[0]\n",
    "    \n",
    "    movie_info = movies_df[movies_df['movieId'] == popular_movie]\n",
    "    if not movie_info.empty:\n",
    "        title = movie_info.iloc[0]['title']\n",
    "        print(f\"Finding movies similar to: {title}\")\n",
    "        \n",
    "        similar_items = inference.find_similar_items(popular_movie, top_k=5)\n",
    "        \n",
    "        print(\"\\nSimilar movies:\")\n",
    "        for movie_id, similarity in similar_items:\n",
    "            similar_info = movies_df[movies_df['movieId'] == movie_id]\n",
    "            if not similar_info.empty:\n",
    "                similar_title = similar_info.iloc[0]['title']\n",
    "                genres = similar_info.iloc[0]['genres']\n",
    "                print(f\"  {similar_title} (Similarity: {similarity:.3f})\")\n",
    "                print(f\"    Genres: {genres}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Full NCF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, let's create a full NCF model\n",
    "# (You can train this for better performance)\n",
    "full_ncf_model = NeuralCollaborativeFiltering(\n",
    "    num_users=model_config['num_users'],\n",
    "    num_items=model_config['num_items'],\n",
    "    embedding_dim=64,\n",
    "    hidden_layers=[128, 64],\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(f\"Full NCF model parameters: {sum(p.numel() for p in full_ncf_model.parameters()):,}\")\n",
    "print(f\"Simple NCF model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nFull NCF has {sum(p.numel() for p in full_ncf_model.parameters()) / sum(p.numel() for p in model.parameters()):.1f}x more parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "To improve the model further:\n",
    "\n",
    "1. **Train longer**: Increase epochs to 50+ for better convergence\n",
    "2. **Try different architectures**: \n",
    "   - Full NCF model with GMF + MLP\n",
    "   - DeepNCF with genre information\n",
    "3. **Hyperparameter tuning**: Experiment with embedding dimensions, learning rates\n",
    "4. **Data augmentation**: Add negative sampling, use implicit feedback\n",
    "5. **Ensemble methods**: Combine multiple models for better performance\n",
    "\n",
    "## Summary\n",
    "\n",
    "This tutorial showed:\n",
    "- ✅ Data loading and preprocessing for NCF\n",
    "- ✅ Training a Simple NCF model\n",
    "- ✅ Evaluating model performance\n",
    "- ✅ Making personalized recommendations\n",
    "- ✅ Finding similar items\n",
    "- ✅ Model analysis and comparison\n",
    "\n",
    "The Neural Collaborative Filtering approach provides a powerful foundation for learning complex user-item interaction patterns that traditional collaborative filtering might miss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}