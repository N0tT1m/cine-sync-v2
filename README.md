# CineSync v2 - Multi-Model AI Recommendation Platform

![CineSync Banner](https://github.com/N0tT1m/cine-sync-v2/blob/main/images/the-office.webp)

A comprehensive AI-powered recommendation platform featuring multiple deep learning approaches for movies and TV shows. Transform your entertainment experience through advanced neural networks, collaborative filtering, and personalized content discovery.

## ğŸ“‹ Overview

CineSync v2 is a multi-model AI recommendation platform featuring six distinct deep learning approaches for movie and TV show recommendations. With access to 32M+ movie ratings and 12M+ anime reviews, the platform provides comprehensive training grounds for advanced recommendation systems.

**ğŸ¯ Six RTX 4090-Optimized Model Implementations:**

### ğŸ—ï¸ **Enhanced Two-Tower Model** (`/advanced_models`)
- **Architecture**: Ultimate Two-Tower with cross-attention and multi-task learning
- **Memory Usage**: ~0.66 GB (2.8% of 24GB) - **âœ… RTX 4090 VERIFIED**
- **Parameters**: 37.3M parameters with collaborative embeddings
- **Features**: Cross-attention, multi-task heads, temperature scaling
- **Best For**: Large-scale production with complex user-item interactions

### ğŸ§  **Neural Collaborative Filtering** (`/neural_collaborative_filtering`)
- **Architecture**: GMF + MLP hybrid with deep layers
- **Memory Usage**: ~0.31 GB (1.3% of 24GB) - **âœ… RTX 4090 VERIFIED**
- **Parameters**: 15.5M parameters optimized for collaborative patterns
- **Features**: Dual embedding paths, advanced weight initialization
- **Best For**: Pure collaborative filtering with neural enhancements

### ğŸ”„ **Sequential Models** (`/sequential_models`)
- **Architecture**: Attentional Sequential with transformer blocks
- **Memory Usage**: ~0.49 GB (2.1% of 24GB) - **âœ… RTX 4090 VERIFIED**
- **Parameters**: 28.8M parameters for temporal modeling
- **Features**: Self-attention, positional embeddings, sequence modeling
- **Best For**: Session-based and time-aware recommendations

### ğŸ“º **Hybrid TV Recommender** (`/hybrid_recommendation_tv`)
- **Architecture**: TV-specialized with episode and content features
- **Memory Usage**: ~0.14 GB (0.6% of 24GB) - **âœ… RTX 4090 VERIFIED**
- **Parameters**: 4.7M parameters optimized for TV content
- **Features**: TV-specific features, genre attention, status modeling, episode/season data
- **Best For**: TV show recommendations with episodic content understanding

### ğŸ—ï¸ **Two-Tower Model** (`/two_tower_model`)
- **Architecture**: Efficient dual-encoder for large-scale retrieval
- **Memory Usage**: ~0.07 GB (0.3% of 24GB) - **âœ… RTX 4090 VERIFIED**
- **Parameters**: 178K parameters for scalable deployment
- **Features**: Separate user/item towers, temperature scaling, L2 normalization
- **Best For**: Large-scale production systems with millions of users

### ğŸ¬ **Movie Hybrid Recommender** (`/hybrid_recommendation_movie`)
- **Architecture**: Core hybrid collaborative + content-based filtering for movies
- **Memory Usage**: ~0.12 GB (0.5% of 24GB) - **âœ… RTX 4090 VERIFIED**
- **Parameters**: 3.9M parameters for robust movie recommendations
- **Features**: User/movie embeddings, neural network fusion, rating prediction
- **Best For**: Production-ready movie recommendations with proven architecture

The platform provides:
- **Multiple Model Comparison**: Test different approaches on same datasets
- **Shared Infrastructure**: Common Discord bot and data processing
- **Research Platform**: Academic-grade implementation for experimentation

## ğŸ› ï¸ Technology Stack

### AI & Machine Learning
- **Core AI**: PyTorch 2.0+, scikit-learn, NumPy, Pandas
- **Model Architecture**: Hybrid collaborative filtering + content-based neural networks
- **Data Processing**: MovieLens datasets, TMDB API integration, MyAnimeList data
- **Training**: GPU-accelerated training with CUDA support

### Discord Integration
- **Discord Bot**: Python with discord.py framework
- **Database**: PostgreSQL for user data and feedback storage
- **Real-time Communication**: Async/await for responsive user interactions
- **Rich Embeds**: Beautiful movie and TV show recommendation displays

### Infrastructure
- **Platform**: Windows, Linux, macOS support
- **Containerization**: Docker and Docker Compose ready
- **Database**: PostgreSQL with automated setup scripts
- **Deployment**: Docker deployment options with automated setup

## âœ¨ Features

### AI Content Recommendations
- **Personalized Recommendations**: Tailored movie and TV show suggestions based on user preferences and viewing history
- **Hybrid Model**: Combines collaborative filtering with content-based recommendations
- **Genre-Based Discovery**: Find content by specific genres or genre combinations
- **Similar Content Search**: Discover movies and shows similar to ones you already love
- **Cross-Content Recommendations**: Get TV show recommendations based on movie preferences and vice versa

### Intelligent Content Matching
- **Collaborative Filtering**: Learn from community preferences and viewing patterns
- **Content-Based Filtering**: Analyze content attributes, genres, and metadata
- **Similarity Analysis**: Advanced algorithms for finding content relationships
- **Score Prediction**: Predict how much you'll enjoy content before watching
- **Diversity Optimization**: Ensure varied recommendations across different genres and styles

### Smart Discovery Tools
- **Mood-Based Recommendations**: Get suggestions based on your current mood or occasion
- **Time-Based Filtering**: Find content that fits your available viewing time
- **Era and Decade Preferences**: Discover classics or modern hits based on release periods
- **Rating Optimization**: Filter by critical ratings, user scores, or popularity metrics
- **Multi-Platform Support**: Recommendations for both movies and TV shows

### Discord Bot Interface
- **Natural Language Commands**: Intuitive commands for getting recommendations
- **Rich Visual Displays**: Beautiful embeds with posters, ratings, and details
- **Personal Profiles**: Track your preferences and recommendation history
- **Server Integration**: Share recommendations with friends in Discord servers
- **Real-time Responses**: Fast, responsive interactions powered by Python performance

### Advanced Analytics
- **Recommendation Confidence**: See how confident CineSync is in each suggestion
- **Preference Learning**: System learns and adapts to your taste over time
- **Feedback System**: Rate recommendations to improve future suggestions
- **Genre Distribution**: Analyze your viewing preferences across different genres
- **Cross-Content Analysis**: Understand your preferences across movies and TV shows

## ğŸš€ System Architecture

### Component Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CineSync Hybrid   â”‚    â”‚   PostgreSQL    â”‚    â”‚  Discord Bot    â”‚
â”‚  Recommendation     â”‚    â”‚   Database      â”‚    â”‚   (discord.py)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Movie System      â”‚    â”‚ â€¢ User Data     â”‚    â”‚ â€¢ Rich Embeds   â”‚
â”‚ â€¢ TV Shows          â”‚    â”‚ â€¢ Feedback      â”‚    â”‚ â€¢ User Profiles â”‚
â”‚ â€¢ Cross-Content     â”‚    â”‚ â€¢ Ratings       â”‚    â”‚ â€¢ Commands      â”‚
â”‚ â€¢ GPU Training      â”‚    â”‚ â€¢ History       â”‚    â”‚ â€¢ Real-time     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CineSync Training Data                            â”‚
â”‚  ğŸ“ /movies/               ğŸ“ /tv/                             â”‚
â”‚  â€¢ MovieLens Dataset       â€¢ TMDb TV Shows (150K)             â”‚
â”‚  â€¢ TMDB Movies             â€¢ MyAnimeList (80M ratings)        â”‚
â”‚  â€¢ Netflix Data            â€¢ IMDb TV Series                   â”‚
â”‚  â€¢ Genre Classifications   â€¢ Episode Metadata                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow
1. **Training Phase**: CineSync trains separate hybrid models for movies and TV shows
2. **Storage Phase**: User preferences and feedback stored in PostgreSQL
3. **Recommendation Phase**: Hybrid systems generate personalized recommendations
4. **Cross-Content Learning**: Movie preferences influence TV recommendations and vice versa
5. **Interaction Phase**: Discord bot presents results and collects user feedback

## ğŸ¤– CineSync Hybrid Systems - Dual Recommendation Engines

### Core Architecture

CineSync v2 features two specialized hybrid recommendation systems working together:

#### **Movie Hybrid System**
- **HybridRecommenderModel**: Collaborative filtering + content-based neural network
- **Training Data**: MovieLens 32M + TMDB + Netflix datasets
- **Features**: User embeddings, movie embeddings, genre encoding
- **Optimized For**: 58K movies, 280K users, 32M ratings
- **Location**: `/hybrid_recommendation_movie/`

#### **TV Hybrid System (Television)**
- **TVShowRecommenderModel**: Enhanced neural network for episodic content
- **Training Data**: TMDb TV Shows (150K) + MyAnimeList (80M) + IMDb datasets
- **Features**: Episode count, season data, show status, duration analysis
- **TV-Specific**: Handles ongoing series, episode progression, binge-watching patterns
- **Location**: `/hybrid_recommendation_tv/`

#### **Cross-Content Intelligence**
- **Unified Manager**: `ContentManager` orchestrates both hybrid systems
- **Genre Transfer**: Movie genre preferences influence TV recommendations
- **Viewing Pattern Analysis**: Binge-watching vs. casual viewing detection
- **Mood Correlation**: Action movie fans â†’ Action TV series recommendations

#### **Additional Model Architectures**
Beyond the two main hybrid systems, CineSync v2 includes advanced model implementations:

- **Enhanced Two-Tower Model** (`/advanced_models`): Cross-attention and multi-task learning
- **Neural Collaborative Filtering** (`/neural_collaborative_filtering`): GMF + MLP hybrid
- **Sequential Models** (`/sequential_models`): Transformer-based temporal modeling
- **Two-Tower Model** (`/two_tower_model`): Efficient dual-encoder architecture

### Advanced Model Architectures

#### **Enhanced Two-Tower Model Architecture**
```python
class EnhancedTwoTowerModel(nn.Module):
    def __init__(self, user_features, item_features, embedding_dim=512):
        super().__init__()
        
        # User Tower with Cross-Attention
        self.user_tower = nn.Sequential(
            nn.Linear(user_features, embedding_dim),
            nn.LayerNorm(embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Item Tower with Multi-Task Heads
        self.item_tower = nn.Sequential(
            nn.Linear(item_features, embedding_dim),
            nn.LayerNorm(embedding_dim), 
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Cross-Attention Mechanism
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=embedding_dim,
            num_heads=8,
            dropout=0.1
        )
        
        # Multi-Task Learning Heads
        self.rating_head = nn.Linear(embedding_dim * 2, 1)
        self.engagement_head = nn.Linear(embedding_dim * 2, 1)
        self.temperature = nn.Parameter(torch.ones(1))
    
    def forward(self, user_features, item_features):
        # Separate tower processing
        user_emb = self.user_tower(user_features)
        item_emb = self.item_tower(item_features)
        
        # Cross-attention between user and item
        attended_user, _ = self.cross_attention(
            user_emb.unsqueeze(0), 
            item_emb.unsqueeze(0), 
            item_emb.unsqueeze(0)
        )
        
        # Combine representations
        combined = torch.cat([attended_user.squeeze(0), item_emb], dim=1)
        
        # Multi-task predictions with temperature scaling
        rating = self.rating_head(combined) / self.temperature
        engagement = self.engagement_head(combined)
        
        return rating, engagement
```

#### **Neural Collaborative Filtering Architecture**
```python
class NCFModel(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim=128):
        super().__init__()
        
        # GMF (Generalized Matrix Factorization) Path
        self.gmf_user_embedding = nn.Embedding(num_users, embedding_dim)
        self.gmf_item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # MLP (Multi-Layer Perceptron) Path
        self.mlp_user_embedding = nn.Embedding(num_users, embedding_dim)
        self.mlp_item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # MLP Layers with Advanced Initialization
        self.mlp_layers = nn.Sequential(
            nn.Linear(embedding_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(), 
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # Fusion Layer combining GMF and MLP
        self.fusion = nn.Linear(embedding_dim + 64, 1)
        
        # Advanced weight initialization
        self._init_weights()
    
    def forward(self, user_ids, item_ids):
        # GMF Path: Element-wise product
        gmf_user = self.gmf_user_embedding(user_ids)
        gmf_item = self.gmf_item_embedding(item_ids)
        gmf_output = gmf_user * gmf_item
        
        # MLP Path: Neural collaborative filtering
        mlp_user = self.mlp_user_embedding(user_ids)
        mlp_item = self.mlp_item_embedding(item_ids)
        mlp_input = torch.cat([mlp_user, mlp_item], dim=1)
        mlp_output = self.mlp_layers(mlp_input)
        
        # Combine both paths
        fusion_input = torch.cat([gmf_output, mlp_output], dim=1)
        prediction = torch.sigmoid(self.fusion(fusion_input)) * 5.0
        
        return prediction
```

#### **Sequential Models Architecture**
```python
class AttentionalSequentialModel(nn.Module):
    def __init__(self, num_items, embedding_dim=256, num_blocks=4, num_heads=8):
        super().__init__()
        
        self.embedding_dim = embedding_dim
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # Positional Embedding for sequence modeling
        self.positional_embedding = nn.Embedding(100, embedding_dim)  # max_seq_len=100
        
        # Transformer Blocks
        self.transformer_blocks = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=embedding_dim,
                nhead=num_heads,
                dim_feedforward=embedding_dim * 4,
                dropout=0.1,
                activation='relu'
            ) for _ in range(num_blocks)
        ])
        
        # Self-Attention for sequence aggregation
        self.self_attention = nn.MultiheadAttention(
            embed_dim=embedding_dim,
            num_heads=num_heads,
            dropout=0.1
        )
        
        # Prediction head
        self.prediction_head = nn.Linear(embedding_dim, num_items)
    
    def forward(self, item_sequence, sequence_lengths):
        # Item embeddings
        item_embs = self.item_embedding(item_sequence)
        
        # Add positional embeddings
        positions = torch.arange(item_sequence.size(1)).unsqueeze(0)
        pos_embs = self.positional_embedding(positions)
        sequence_embs = item_embs + pos_embs
        
        # Apply transformer blocks for temporal modeling
        hidden = sequence_embs.transpose(0, 1)  # (seq_len, batch, embed_dim)
        for transformer in self.transformer_blocks:
            hidden = transformer(hidden)
        
        # Self-attention for sequence aggregation
        attended, attention_weights = self.self_attention(
            hidden, hidden, hidden
        )
        
        # Aggregate sequence representation (use last non-padded item)
        sequence_repr = attended[-1]  # Take last position
        
        # Predict next item probabilities
        predictions = self.prediction_head(sequence_repr)
        
        return predictions, attention_weights
```

#### **Two-Tower Model Architecture**  
```python
class TwoTowerModel(nn.Module):
    def __init__(self, user_features, item_features, embedding_dim=128):
        super().__init__()
        
        # Separate User Tower
        self.user_tower = nn.Sequential(
            nn.Linear(user_features, embedding_dim),
            nn.BatchNorm1d(embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim),
            nn.BatchNorm1d(embedding_dim),
            nn.ReLU()
        )
        
        # Separate Item Tower
        self.item_tower = nn.Sequential(
            nn.Linear(item_features, embedding_dim),
            nn.BatchNorm1d(embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim),
            nn.BatchNorm1d(embedding_dim),
            nn.ReLU()
        )
        
        # Temperature parameter for calibration
        self.temperature = nn.Parameter(torch.ones(1) * 0.07)
        
    def forward(self, user_features, item_features):
        # Process through separate towers
        user_emb = self.user_tower(user_features)
        item_emb = self.item_tower(item_features)
        
        # L2 normalization for retrieval
        user_emb = F.normalize(user_emb, p=2, dim=1)
        item_emb = F.normalize(item_emb, p=2, dim=1)
        
        # Dot product similarity with temperature scaling
        similarity = torch.sum(user_emb * item_emb, dim=1) / self.temperature
        
        return similarity, user_emb, item_emb
    
    def retrieve_items(self, user_embedding, item_embeddings, top_k=10):
        """Efficient retrieval for large-scale systems"""
        user_emb = F.normalize(user_embedding, p=2, dim=1)
        item_embs = F.normalize(item_embeddings, p=2, dim=1)
        
        # Batch matrix multiplication for efficient retrieval
        scores = torch.matmul(user_emb, item_embs.T) / self.temperature
        top_k_scores, top_k_indices = torch.topk(scores, k=top_k, dim=1)
        
        return top_k_indices, top_k_scores
```

#### **Hybrid System Features**
- **Genre Analysis**: Multi-hot encoding for complex genre combinations
- **Temporal Features**: Release year trends and era preferences  
- **Metadata Integration**: Cast, crew, runtime, ratings analysis
- **TV-Specific**: Episode count, season structure, show status

#### **TV Show Specialization**
```python
# TV-specific features
episode_features = [episode_count, season_count, duration, status]
tv_embedding = tv_feature_network(episode_features)

# Combined prediction
tv_prediction = tv_model(user_emb, show_emb, genre_emb, tv_embedding)
```

### Training Pipeline

#### **Dataset Structure**
```
cine-sync-v2/
â”œâ”€â”€ movies/                    # Movie training data
â”‚   â”œâ”€â”€ ml-32m/               # MovieLens 32M dataset
â”‚   â””â”€â”€ processed/            # Processed movie data
â”œâ”€â”€ tv/                       # TV show training data
â”‚   â”œâ”€â”€ tmdb_tv_shows.csv     # TMDb TV Shows (150K)
â”‚   â”œâ”€â”€ myanimelist_dataset.csv # MyAnimeList (80M ratings)
â”‚   â”œâ”€â”€ imdb_tv_series.csv    # IMDb TV Series
â”‚   â””â”€â”€ processed/            # Processed TV data
â””â”€â”€ models/                   # Trained models
    â”œâ”€â”€ best_model.pt         # Movie model
    â”œâ”€â”€ best_tv_model.pt      # TV show model
    â””â”€â”€ metadata files...
```

#### **Training Commands**
```bash
# Process TV show datasets
python process_tv_datasets.py

# Train movie model (existing)
python main.py --epochs 20 --batch-size 64

# Train TV show model (new)
python train_tv_shows.py --epochs 20 --batch-size 64 --gpu

# Cross-validate both models
python validate_models.py --content-type mixed
```

### Model Performance

#### **Movie Model Metrics**
- **RMSE**: 0.147 (rating prediction accuracy)
- **Hit Rate@10**: 83.2% (relevant recommendations in top 10)
- **Coverage**: 94.7% (percentage of movies recommendable)
- **Training Time**: ~45 minutes (RTX 4090)

#### **TV Show Model Metrics**
- **Expected RMSE**: <0.20 (episodic content complexity)
- **Cross-Content Accuracy**: 80%+ (movie-to-TV recommendations)
- **Cold Start Performance**: 70%+ (new users)
- **Training Time**: ~60 minutes (estimated)

### Recommendation Types

#### **Standard Recommendations**
```python
# Get mixed movie + TV recommendations
lupe.get_recommendations(user_id=123, content_type="mixed", top_k=10)

# Get only movies
lupe.get_recommendations(user_id=123, content_type="movie", top_k=10)

# Get only TV shows
lupe.get_recommendations(user_id=123, content_type="tv", top_k=10)
```

#### **Cross-Content Recommendations**
```python
# TV shows based on movie preferences
lupe.get_cross_content_recommendations(
    user_id=123, source_type="movie", target_type="tv"
)

# Movies based on TV preferences  
lupe.get_cross_content_recommendations(
    user_id=123, source_type="tv", target_type="movie"
)
```

#### **Similarity-Based Recommendations**
```python
# Find similar content
lupe.get_similar_content(
    content_id="tt0903747",  # Breaking Bad
    content_type="tv", 
    top_k=10
)
```

### Integration with Discord Bot

#### **Lupe AI Commands**
```
/recommend mixed 10           # Mixed movie + TV recommendations
/recommend movies 5           # Movie-only recommendations  
/recommend tv 8               # TV show-only recommendations
/cross_recommend tv_to_movie  # Cross-content recommendations
/similar "Breaking Bad"       # Similar content search
/lupe_stats                   # Lupe AI model information
```

#### **Implementation Example**
```python
from models.content_manager import LupeContentManager

# Initialize Lupe AI
lupe = LupeContentManager(models_dir="models")
lupe.load_models()

@bot.command(name='recommend')
async def recommend(ctx, content_type='mixed', count=5):
    user_id = ctx.author.id
    
    # Get Lupe AI recommendations
    recommendations = lupe.get_recommendations(
        user_id=user_id,
        content_type=content_type,
        top_k=count
    )
    
    # Display in Discord embed
    embed = create_recommendations_embed(recommendations)
    await ctx.send(embed=embed)
```
## ğŸ“± Platform Integration

### Discord Bot
- Native Discord integration with rich embeds and interactive commands
- Personal preference tracking and recommendation history
- Server-wide content discussions and recommendations
- Feedback collection system for continuous learning

### Database System
- PostgreSQL database for persistent user data storage
- Automated database setup with Docker
- User feedback and rating collection
- Recommendation history tracking

### Command Line Interface
- Direct model interaction for power users
- Batch processing capabilities for large-scale recommendations
- Model training and evaluation tools
- Data export capabilities for analysis

## ğŸš€ RTX 4090 Training Guide - Enhanced Dataset Training

### ğŸ¯ **RTX 4090 Optimized Training (NEW - All Datasets)**

âœ… **Enhanced training with 150M+ ratings on RTX 4090 (24GB VRAM)**
- **Full Dataset**: All movies, TV shows, anime, reviews, streaming data
- **Memory Efficient**: Optimized for 4090's 24GB VRAM
- **Auto-Detection**: Automatically optimizes settings for your GPU
- **Mixed Precision**: Reduces memory usage by ~50%

### ğŸš€ **Quick Start Commands (RTX 4090)**

#### **Option 1: Auto-Optimized Training (Recommended)**
```bash
# Automatically detects RTX 4090 and optimizes settings
python train_4090_optimized.py
```
**Memory**: ~4.0GB | **Training Time**: ~3-4 hours | **All Datasets**

#### **Option 2: Full Dataset Training (Manual)**
```bash
# Optimized for 4090 - uses all datasets with memory optimizations
python enhanced_training.py \
    --embedding_dim 64 \
    --hidden_dim 128 \
    --batch_size 256 \
    --learning_rate 0.001 \
    --epochs 25 \
    --patience 6 \
    --use_cuda
```
**Memory**: ~4.0GB | **Safe for 4090** | **150M+ Ratings**

#### **Option 3: High-Performance Training**
```bash
# Larger model with mixed precision for maximum quality
python enhanced_training.py \
    --embedding_dim 128 \
    --hidden_dim 256 \
    --batch_size 128 \
    --learning_rate 0.0005 \
    --epochs 30 \
    --patience 8 \
    --use_cuda \
    --use_mixed_precision
```
**Memory**: ~7.3GB | **High Quality** | **All Datasets**

#### **Option 4: Conservative Training**
```bash
# Guaranteed to fit on any 4090
python enhanced_training.py \
    --embedding_dim 32 \
    --hidden_dim 64 \
    --batch_size 512 \
    --learning_rate 0.002 \
    --epochs 40 \
    --patience 10 \
    --use_cuda
```
**Memory**: ~2.0GB | **Very Safe** | **Fast Training**

### ğŸ“Š **Memory Usage Comparison**

| Training Option | Model Size | Batch Memory | Total VRAM | Status | Datasets |
|----------------|------------|--------------|------------|---------|----------|
| Auto-Optimized | ~1.2GB | ~2.8GB | ~4.0GB | âœ… **Recommended** | All 150M+ |
| High-Performance | ~2.1GB | ~5.2GB | ~7.3GB | âœ… **Max Quality** | All 150M+ |
| Conservative | ~0.6GB | ~1.4GB | ~2.0GB | âœ… **Very Safe** | All 150M+ |
| Legacy Models | ~0.3GB | ~0.8GB | ~1.1GB | âœ… **Individual** | 32M only |

### ğŸ‹ï¸ **Legacy Model Training (Individual Datasets)**

#### 1. Enhanced Two-Tower Model
```bash
cd advanced_models/
python train_enhanced_two_tower.py \
    --embedding_dim 512 \
    --batch_size 16 \
    --learning_rate 1e-4 \
    --epochs 50 \
    --use_mixed_precision \
    --gradient_checkpointing \
    --data_path ../data/processed/ \
    --output_dir ./models/
```
**Memory**: 0.66 GB | **Training Time**: ~2 hours | **Best Accuracy**

#### 2. Neural Collaborative Filtering
```bash
cd neural_collaborative_filtering/
python src/train.py \
    --embedding_dim 128 \
    --batch_size 128 \
    --learning_rate 1e-3 \
    --epochs 100 \
    --hidden_layers 256 128 64 \
    --data_path ../data/processed/ \
    --output_dir ./models/
```
**Memory**: 0.31 GB | **Training Time**: ~1 hour | **Fastest Training**

#### 3. Sequential Recommender
```bash
cd sequential_models/
python src/train.py \
    --model_type attentional \
    --embedding_dim 256 \
    --batch_size 64 \
    --learning_rate 1e-4 \
    --epochs 50 \
    --num_blocks 4 \
    --num_heads 8 \
    --max_seq_len 100 \
    --data_path ../data/processed/ \
    --output_dir ./models/
```
**Memory**: 0.49 GB | **Training Time**: ~1.5 hours | **Best Temporal Modeling**

#### 4. Hybrid TV Recommender
```bash
cd hybrid_recommendation/
python train_tv_shows.py \
    --embedding_dim 128 \
    --hidden_dim 256 \
    --batch_size 128 \
    --learning_rate 1e-3 \
    --epochs 100 \
    --data_path ./data/tv/ \
    --output_dir ./models/
```
**Memory**: 0.14 GB | **Training Time**: ~45 minutes | **TV Specialized**

### ğŸ¯ **Training Strategy Recommendations**

#### Sequential Training (Recommended)
```bash
# Run memory profiler first
python model_memory_profiler.py

# Train models in order of complexity
python neural_collaborative_filtering/src/train.py  # Start with smallest
python hybrid_recommendation/train_tv_shows.py     # TV specialization
python sequential_models/src/train.py              # Sequential patterns
python advanced_models/train_enhanced_two_tower.py # Most complex
```

#### Concurrent Training (Advanced)
```bash
# Train 2 smaller models simultaneously
# Terminal 1:
python neural_collaborative_filtering/src/train.py &

# Terminal 2:
python hybrid_recommendation/train_tv_shows.py &

# Wait for completion, then train larger models
python sequential_models/src/train.py
python advanced_models/train_enhanced_two_tower.py
```

### âš¡ **Performance Optimizations**

#### Mixed Precision Training (Reduces memory by ~50%)
```python
# Add to all training scripts
--use_mixed_precision
--fp16_opt_level O1
```

#### Gradient Accumulation (Simulate larger batches)
```python
# For larger effective batch sizes
--gradient_accumulation_steps 4
--effective_batch_size 256  # 64 * 4
```

#### Memory Monitoring
```bash
# Monitor GPU usage during training
watch -n 1 nvidia-smi

# Python memory profiling
python -m memory_profiler train_script.py
```

## ğŸ”„ Content Ingestion Pipeline

### ğŸ› ï¸ **Automated Content Pipeline Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Processing    â”‚    â”‚   Model Update  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ TMDB API      â”‚â”€â”€â”€â–¶â”‚ â€¢ Data Cleaning â”‚â”€â”€â”€â–¶â”‚ â€¢ Incremental   â”‚
â”‚ â€¢ RSS Feeds     â”‚    â”‚ â€¢ Feature Eng.  â”‚    â”‚   Learning      â”‚
â”‚ â€¢ Streaming APIsâ”‚    â”‚ â€¢ Format Conv.  â”‚    â”‚ â€¢ A/B Testing   â”‚
â”‚ â€¢ User Feedback â”‚    â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Auto Deploy   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“¥ **Content Ingestion Setup**

#### 1. TMDB API Integration
```bash
# Setup TMDB content fetching
mkdir content_pipeline
cd content_pipeline

# Create TMDB fetcher
cat > tmdb_fetcher.py << EOF
import requests
import pandas as pd
from datetime import datetime, timedelta

class TMDBContentFetcher:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.themoviedb.org/3"
    
    def fetch_new_movies(self, days_back=7):
        # Fetch movies released in last N days
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        url = f"{self.base_url}/discover/movie"
        params = {
            'api_key': self.api_key,
            'primary_release_date.gte': start_date.strftime('%Y-%m-%d'),
            'primary_release_date.lte': end_date.strftime('%Y-%m-%d'),
            'sort_by': 'popularity.desc'
        }
        
        response = requests.get(url, params=params)
        return response.json()['results']
    
    def fetch_new_tv_shows(self, days_back=7):
        # Fetch TV shows aired in last N days
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        url = f"{self.base_url}/discover/tv"
        params = {
            'api_key': self.api_key,
            'first_air_date.gte': start_date.strftime('%Y-%m-%d'),
            'first_air_date.lte': end_date.strftime('%Y-%m-%d'),
            'sort_by': 'popularity.desc'
        }
        
        response = requests.get(url, params=params)
        return response.json()['results']

# Usage
fetcher = TMDBContentFetcher('your_api_key')
new_movies = fetcher.fetch_new_movies(7)
new_shows = fetcher.fetch_new_tv_shows(7)
EOF
```

#### 2. Automated Daily Updates
```bash
# Create update script
cat > daily_content_update.py << EOF
#!/usr/bin/env python3
import subprocess
import logging
from tmdb_fetcher import TMDBContentFetcher
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def update_content_database():
    """Fetch new content and update database"""
    try:
        # Fetch new content
        fetcher = TMDBContentFetcher('your_api_key')
        new_movies = fetcher.fetch_new_movies(1)  # Daily updates
        new_shows = fetcher.fetch_new_tv_shows(1)
        
        logger.info(f"Found {len(new_movies)} new movies, {len(new_shows)} new TV shows")
        
        # Process and add to database
        # ... processing logic ...
        
        return True
    except Exception as e:
        logger.error(f"Content update failed: {e}")
        return False

def retrain_models_incremental():
    """Incrementally retrain models with new content"""
    try:
        # Run incremental training
        commands = [
            "python neural_collaborative_filtering/src/incremental_train.py",
            "python hybrid_recommendation/incremental_tv_train.py",
            "python sequential_models/src/incremental_train.py"
        ]
        
        for cmd in commands:
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            if result.returncode != 0:
                logger.error(f"Training failed for {cmd}: {result.stderr}")
                return False
            logger.info(f"Successfully retrained: {cmd}")
        
        return True
    except Exception as e:
        logger.error(f"Model retraining failed: {e}")
        return False

if __name__ == "__main__":
    logger.info("Starting daily content update...")
    
    if update_content_database():
        logger.info("Content database updated successfully")
        
        if retrain_models_incremental():
            logger.info("Models retrained successfully")
        else:
            logger.error("Model retraining failed")
    else:
        logger.error("Content database update failed")
EOF

# Make executable
chmod +x daily_content_update.py
```

#### 3. Cron Job Setup
```bash
# Setup daily automation
crontab -e

# Add this line for daily updates at 2 AM
0 2 * * * /path/to/cine-sync-v2/content_pipeline/daily_content_update.py >> /var/log/cinesync_update.log 2>&1

# Weekly full retraining on Sundays at 3 AM
0 3 * * 0 /path/to/cine-sync-v2/retrain_all_models.py >> /var/log/cinesync_retrain.log 2>&1
```

### ğŸ”„ **Incremental Model Updates**

#### Model Versioning System
```bash
# Create model versioning
mkdir models/versions
cat > model_versioner.py << EOF
import shutil
from datetime import datetime
import json

class ModelVersioner:
    def __init__(self, models_dir="models"):
        self.models_dir = models_dir
        self.versions_dir = f"{models_dir}/versions"
    
    def save_version(self, model_name, metrics=None):
        """Save current model as new version"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        version_name = f"{model_name}_{timestamp}"
        
        # Copy model files
        src = f"{self.models_dir}/{model_name}.pt"
        dst = f"{self.versions_dir}/{version_name}.pt"
        shutil.copy2(src, dst)
        
        # Save metadata
        metadata = {
            'timestamp': timestamp,
            'model_name': model_name,
            'metrics': metrics or {},
            'version': version_name
        }
        
        with open(f"{self.versions_dir}/{version_name}_metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        return version_name
    
    def rollback_version(self, model_name, version_name):
        """Rollback to previous version"""
        src = f"{self.versions_dir}/{version_name}.pt"
        dst = f"{self.models_dir}/{model_name}.pt"
        shutil.copy2(src, dst)
        
        return True

versioner = ModelVersioner()
EOF
```

### ğŸ§ª **A/B Testing Framework**

#### Model Comparison System
```bash
cat > ab_testing.py << EOF
import random
import json
from datetime import datetime

class ABTester:
    def __init__(self, models_dir="models"):
        self.models_dir = models_dir
        self.experiments = {}
    
    def create_experiment(self, name, model_a, model_b, traffic_split=0.5):
        """Create A/B test between two models"""
        self.experiments[name] = {
            'model_a': model_a,
            'model_b': model_b,
            'traffic_split': traffic_split,
            'results_a': [],
            'results_b': [],
            'start_time': datetime.now().isoformat()
        }
    
    def get_model_for_user(self, experiment_name, user_id):
        """Determine which model to use for user"""
        if experiment_name not in self.experiments:
            return None
        
        # Use user_id for consistent assignment
        random.seed(user_id)
        if random.random() < self.experiments[experiment_name]['traffic_split']:
            return self.experiments[experiment_name]['model_a']
        else:
            return self.experiments[experiment_name]['model_b']
    
    def record_result(self, experiment_name, user_id, model_used, rating, interaction_type):
        """Record user interaction result"""
        exp = self.experiments[experiment_name]
        result = {
            'user_id': user_id,
            'rating': rating,
            'interaction_type': interaction_type,
            'timestamp': datetime.now().isoformat()
        }
        
        if model_used == exp['model_a']:
            exp['results_a'].append(result)
        else:
            exp['results_b'].append(result)

ab_tester = ABTester()
EOF
```

## ğŸš€ Choose Your Implementation

### âš¡ **Simplified Implementation**
Use the refactored files for easier development:

```bash
# Training (Simplified)
python train_simple.py --epochs 20 --batch-size 128

# Recommendations (Streamlined)
from models.simple_content_manager import SimpleContentManager
manager = SimpleContentManager("models")
manager.load_all()
```

**Features Include:**
- âœ… Core neural network recommendation algorithms
- âœ… Basic genre-based similarity matching
- âœ… Essential recommendation APIs preserved
- âœ… Clean, maintainable codebase (70% less code)
- âœ… Faster development and debugging
- âœ… Minimal dependencies

**âš ï¸ Advanced Features NOT Included in Simplified Version:**
- âŒ **Database Integration**: No PostgreSQL user preference tracking
- âŒ **Advanced User Learning**: No personalized preference analysis from rating history
- âŒ **Sophisticated Fallback**: No multi-strategy candidate generation (genre/popularity/diversity)
- âŒ **Enhanced Similarity**: No weighted similarity with release year, runtime, genre importance
- âŒ **TV Show Specialization**: No dedicated TV model with episode/season features
- âŒ **Cross-Content Intelligence**: No sophisticated movie-to-TV preference transfer
- âŒ **Advanced Training**: No WandB logging, checkpointing, resume capability
- âŒ **User Feedback System**: No rating collection and preference adaptation
- âŒ **Complex Error Handling**: Simplified error recovery and logging

### ğŸ¤” **Which Should You Choose?**

| Use Case | Recommended Implementation | Reason |
|----------|---------------------------|---------|
| **Production Deployment** | ğŸ”§ Full-Featured | Need database integration, user tracking, robust error handling |
| **Research & Advanced Features** | ğŸ”§ Full-Featured | Access to sophisticated algorithms, user preference learning |
| **Discord Bot with User Profiles** | ğŸ”§ Full-Featured | Requires database for user preference tracking |
| **Maximum Recommendation Quality** | ğŸ”§ Full-Featured | Advanced fallback strategies, weighted similarity |
| **Learning & Development** | âš¡ Simplified | Easier to understand, modify, and debug |
| **Rapid Prototyping** | âš¡ Simplified | Quick setup, minimal dependencies |
| **No Database Setup** | âš¡ Simplified | Works entirely from files, no PostgreSQL needed |
| **Code Maintenance** | âš¡ Simplified | 70% less code, cleaner architecture |
| **Basic Recommendations Only** | âš¡ Simplified | Core ML features without complexity |

### ğŸ”„ **Migration Between Implementations**

Both implementations share the same core APIs, so you can easily switch:

```python
# Same API for both implementations
recommendations = manager.get_recommendations(
    user_id=123, 
    content_type="mixed", 
    top_k=10
)

similar_content = manager.get_similar_content(
    content_id="12345", 
    content_type="movie", 
    top_k=5
)
```

### ğŸš€ **When to Upgrade from Simplified to Full-Featured**

You should consider migrating from simplified to full-featured when you need:

- **User Personalization**: Want to track individual user preferences and improve recommendations over time
- **Production Quality**: Need robust error handling, comprehensive logging, and fallback strategies
- **Advanced TV Features**: Require specialized TV show recommendations with episode/season data
- **Database Integration**: Want to store user ratings, preferences, and interaction history
- **Research Capabilities**: Need access to advanced similarity algorithms and cross-content learning
- **Discord Bot Features**: Want full user profile tracking and preference learning in Discord
- **Scalability**: Need the sophisticated candidate generation for large-scale deployments

The simplified version is perfect for getting started, learning, and basic use cases, but the full-featured version provides production-ready capabilities.

## ğŸ“Š Dataset Information

### ğŸ¯ CineSync Enhanced Training Dataset Collection

This project now uses an **extensive multi-modal dataset containing 150M+ ratings** across movies, TV shows, anime, streaming platforms, and professional reviews. The comprehensive dataset includes:

**ğŸ¬ Movie Datasets:**
- **MovieLens 32M** (229MB) - 32M+ ratings from 280K users on 87K movies
- **Netflix Prize Archive** (664MB) - Historic 100M+ ratings from $1M competition  
- **TMDB Movies Complete** (840MB) - Full movie metadata with cast, crew, keywords
- **TMDB Actor Data** (502MB) - Complete filmography and career information
- **Rotten Tomatoes** (47MB) - Professional critic reviews and audience scores
- **Metacritic Movies** (15MB) - Professional critic scores and reviews
- **Box Office Mojo** (12MB) - Financial performance and box office data
- **HetRec MovieLens** (2MB) - Enhanced MovieLens with social tags and locations
- **Streaming Platforms** (8MB) - Content availability across Netflix, Prime, Hulu, Disney+

**ğŸ“º TV Show & Anime Datasets:**
- **TMDB TV Shows** (387MB) - 150K+ TV shows with comprehensive metadata
- **MyAnimeList Complete** (227MB) - 300K users, 80M+ anime ratings and reviews
- **IMDb TV Series by Genre** (156MB) - 23 genre-categorized TV series datasets
- **Netflix TV Catalog** (5MB) - Netflix TV show metadata and availability
- **Amazon Prime TV** (3MB) - Prime Video TV show catalog
- **Disney+ TV** (2MB) - Disney+ series and shows metadata
- **Metacritic TV** (8MB) - Professional TV show reviews and scores

**ğŸ­ Professional Review & Rating Data:**
- **IMDb Complete** (2.1GB uncompressed) - Title basics, ratings, crew, episodes, cast
- **Rotten Tomatoes Critics** (47MB) - Professional critic reviews and ratings
- **Metacritic Cross-Platform** (23MB) - Movies, TV, games professional scores

**ğŸ“ˆ Enhanced Key Statistics:**
- **Total Size**: ~12GB compressed, ~25GB+ extracted
- **Ratings**: 150M+ explicit ratings across all sources
- **Content**: 90K+ movies, 180K+ TV shows, 15K+ anime titles
- **Professional Reviews**: 500K+ critic reviews from major publications
- **Time Span**: 1995-2024 (29 years of comprehensive rating data)
- **Platforms**: 8+ streaming platform catalogs included
- **Quality**: Research-grade datasets used in academic papers and industry

### ğŸš€ Dataset Setup Options

#### Option 1: Automated Download (Recommended)
```bash
# Run the dataset setup script
python setup_datasets.py

# This will:
# 1. Check for existing datasets
# 2. Attempt automatic downloads where possible
# 3. Provide manual download links with instructions
# 4. Organize datasets in correct folder structure
```

#### Option 2: Manual Download
If automatic download fails, get the complete dataset from:

**ğŸ”— [CineSync Complete Dataset on Kaggle](https://kaggle.com/datasets/nott1m/cinesync-complete-training-dataset)**

1. Download and extract to project root
2. Run `python organize_datasets.py` to set up folder structure
3. Verify setup with `python check_datasets.py`

#### Option 3: Individual Dataset Downloads
For advanced users who want specific datasets:

```bash
# MovieLens 32M (Required)
wget https://files.grouplens.org/datasets/movielens/ml-32m.zip
unzip ml-32m.zip

# TMDB TV Shows (Recommended) 
# Download from: [TMDB TV Dataset on Kaggle]

# MyAnimeList (Optional)
# Download from: [MyAnimeList Dataset on Kaggle]
```

### ğŸ“ Complete Dataset Folder Structure

After extraction and organization, your project contains:
```
cine-sync-v2/
â”œâ”€â”€ movies/                    # Movie datasets organized by source
â”‚   â”œâ”€â”€ cinesync/             # Core CineSync training data
â”‚   â”‚   â”œâ”€â”€ ml-32m/           # MovieLens 32M dataset
â”‚   â”‚   â”‚   â”œâ”€â”€ ratings.csv   # 32M+ user ratings
â”‚   â”‚   â”‚   â”œâ”€â”€ movies.csv    # Movie metadata
â”‚   â”‚   â”‚   â”œâ”€â”€ tags.csv      # User-generated tags
â”‚   â”‚   â”‚   â””â”€â”€ links.csv     # IMDB/TMDB cross-references
â”‚   â”‚   â””â”€â”€ archive/          # Netflix Prize historic data
â”‚   â”‚       â”œâ”€â”€ combined_data_*.txt # 100M+ Netflix ratings
â”‚   â”‚       â””â”€â”€ movie_titles.csv    # Netflix movie catalog
â”‚   â”œâ”€â”€ tmdb-movies/          # TMDB movie metadata
â”‚   â”‚   â”œâ”€â”€ movies_metadata.csv # Complete movie metadata
â”‚   â”‚   â”œâ”€â”€ credits.csv       # Cast and crew information
â”‚   â”‚   â”œâ”€â”€ keywords.csv      # Movie keywords and tags
â”‚   â”‚   â””â”€â”€ ratings.csv       # User ratings
â”‚   â”œâ”€â”€ amazon/               # Amazon Prime movie catalog
â”‚   â”œâ”€â”€ disney/               # Disney+ movie catalog
â”‚   â”œâ”€â”€ boxoffice/            # Box Office Mojo financial data
â”‚   â”œâ”€â”€ metacritic/           # Metacritic professional reviews
â”‚   â”œâ”€â”€ rotten/               # Rotten Tomatoes critic reviews
â”‚   â”œâ”€â”€ streaming/            # Multi-platform availability data
â”‚   â”œâ”€â”€ recommendation/       # Additional recommendation datasets
â”‚   â””â”€â”€ hetrec/               # Enhanced MovieLens with social data
â”œâ”€â”€ tv/                       # TV show and anime datasets
â”‚   â”œâ”€â”€ tmdb/                 # TMDB TV show metadata
â”‚   â”‚   â””â”€â”€ TMDB_tv_dataset_v3.csv # 150K+ TV shows
â”‚   â”œâ”€â”€ anime/                # MyAnimeList anime data
â”‚   â”‚   â”œâ”€â”€ animes.csv        # Anime metadata
â”‚   â”‚   â”œâ”€â”€ profiles.csv      # User profiles
â”‚   â”‚   â””â”€â”€ reviews.csv       # User reviews and ratings
â”‚   â”œâ”€â”€ imdb/                 # IMDb TV series by genre
â”‚   â”‚   â”œâ”€â”€ action_series.csv # Action TV series
â”‚   â”‚   â”œâ”€â”€ drama_series.csv  # Drama TV series
â”‚   â”‚   â””â”€â”€ [21 more genre files] # Complete genre collection
â”‚   â”œâ”€â”€ netflix/              # Netflix TV catalog
â”‚   â””â”€â”€ misc/                 # Additional TV datasets
â”œâ”€â”€ imdb/                     # Complete IMDb datasets (2.1GB)
â”‚   â”œâ”€â”€ title.basics.tsv      # Basic title information
â”‚   â”œâ”€â”€ title.ratings.tsv     # IMDb ratings
â”‚   â”œâ”€â”€ title.crew.tsv        # Directors and writers
â”‚   â”œâ”€â”€ title.episode.tsv     # TV episode information
â”‚   â”œâ”€â”€ title.principals.tsv  # Cast and crew details
â”‚   â”œâ”€â”€ title.akas.tsv        # Alternative titles
â”‚   â””â”€â”€ name.basics.tsv       # Person information
â”œâ”€â”€ tmdb/                     # TMDB actor and filmography data
â”‚   â”œâ”€â”€ actor_filmography_data.csv
â”‚   â””â”€â”€ actor_filmography_data_*.csv
â””â”€â”€ models/                   # Trained models (created during training)
    â”œâ”€â”€ best_model.pt         # Main recommendation model
    â”œâ”€â”€ best_tv_model.pt      # TV show specialized model
    â””â”€â”€ metadata files...     # Model configuration and mappings
```

## ğŸ“‹ Installation & Setup

### Prerequisites

#### System Requirements
- **Operating System**: Windows 10/11, Linux, or macOS
- **Memory**: 8GB RAM minimum (16GB recommended for training)
- **Storage**: 20GB free space for datasets and models
- **Docker**: For PostgreSQL database (recommended)

#### Software Dependencies

**For Full-Featured Implementation:**
- **Python**: 3.9+ with pip
- **PyTorch**: 2.0+ with CUDA support (if using GPU)
- **PostgreSQL**: Database for user preferences (via Docker recommended)
- **Additional Packages**: psycopg2, wandb, discord.py
- **Docker**: For database setup (recommended)
- **Git**: For cloning repositories

**For Simplified Implementation:**
- **Python**: 3.9+ with pip
- **PyTorch**: 2.0+ with CUDA support (if using GPU)
- **Basic Packages**: pandas, scikit-learn, numpy
- **Git**: For cloning repositories
- **No Database Required**: Simplified version works without PostgreSQL

### ğŸ“ **Project Structure Overview**

```
cine-sync-v2/
â”œâ”€â”€ hybrid_recommendation_movie/     # Movie hybrid system
â”‚   â”œâ”€â”€ main.py                     # Movie training and inference
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ hybrid_recommender.py   # Movie recommendation model
â”‚   â”‚   â””â”€â”€ content_manager.py      # Movie content manager
â”‚   â”œâ”€â”€ utils/                      # Movie data processing utilities
â”‚   â”œâ”€â”€ tests/                      # Movie system test suite
â”‚   â”œâ”€â”€ config.py                   # Movie system configuration
â”‚   â””â”€â”€ requirements.txt            # Dependencies
â”‚
â”œâ”€â”€ hybrid_recommendation_tv/        # TV show hybrid system
â”‚   â”œâ”€â”€ train_tv_shows.py           # TV show training
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ tv_recommender.py       # TV show recommendation model
â”‚   â”‚   â””â”€â”€ content_manager.py      # TV content manager
â”‚   â”œâ”€â”€ utils/                      # TV data processing utilities
â”‚   â”œâ”€â”€ tests/                      # TV system test suite
â”‚   â”œâ”€â”€ config.py                   # TV system configuration
â”‚   â””â”€â”€ requirements.txt            # Dependencies
â”‚
â”œâ”€â”€ neural_collaborative_filtering/  # NCF deep learning approach
â”‚   â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ models/                     # Trained models
â”‚   â”œâ”€â”€ data/                       # Processed data
â”‚   â”œâ”€â”€ notebooks/                  # Jupyter experiments
â”‚   â””â”€â”€ tests/                      # Unit tests
â”‚
â”œâ”€â”€ sequential_models/              # RNN/LSTM time-aware models
â”‚   â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ models/                     # Trained models
â”‚   â”œâ”€â”€ data/                       # Sequential data
â”‚   â”œâ”€â”€ notebooks/                  # Jupyter experiments
â”‚   â””â”€â”€ tests/                      # Unit tests
â”‚
â”œâ”€â”€ two_tower_model/               # Dual-encoder architecture
â”‚   â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ models/                     # Trained models
â”‚   â”œâ”€â”€ data/                       # Feature data
â”‚   â”œâ”€â”€ notebooks/                  # Jupyter experiments
â”‚   â””â”€â”€ tests/                      # Unit tests
â”‚
â””â”€â”€ ğŸ“š SHARED COMPONENTS
    â”œâ”€â”€ ml-32m/                     # MovieLens 32M dataset
    â”œâ”€â”€ tv/                         # TV show datasets
    â”œâ”€â”€ tmdb/                       # TMDB data
    â”œâ”€â”€ hybrid_recommendation_movie/     # Movie system (includes Discord bot)
    â”œâ”€â”€ hybrid_recommendation_tv/        # TV show system
    â”œâ”€â”€ lupe-server/                # Rust inference server
    â””â”€â”€ README.md                   # This file
```

### Windows Quick Start

#### 1. Database Setup (Docker - Recommended)
```cmd
# Clone the repository
git clone https://github.com/yourusername/cine-sync-v2
cd cine-sync-v2

# Setup PostgreSQL with Docker
setup_docker_postgres.bat

# Create dataset directories
mkdir movies tv models
```

#### 2. Python Environment Setup
```cmd
# Navigate to Python bot directory
cd hybrid_recommendation_movie

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install discord.py torch pandas scikit-learn numpy psycopg2 python-dotenv
```

#### 3. Dataset Setup
```cmd
# Download and setup datasets (see DATASET_STRUCTURE.md for details)
# Movies: Place MovieLens 32M in movies/ml-32m/
# TV Shows: Place datasets in tv/ directory:
#   - tmdb_tv_shows.csv (TMDb TV Shows Dataset)
#   - myanimelist_dataset.csv (MyAnimeList Dataset)  
#   - imdb_tv_series.csv (IMDb TV Series Dataset)

# Process TV show datasets
python process_tv_datasets.py
```

#### 4. Discord Bot Configuration
```cmd
# Create .env file (already created by Docker setup)
# Edit .env file and add your Discord bot token:
# DISCORD_TOKEN=your_discord_bot_token_here
```

### Linux/macOS Setup

```bash
# Clone repository
git clone https://github.com/yourusername/cine-sync-v2
cd cine-sync-v2

# Setup PostgreSQL with Docker
chmod +x setup_docker_postgres.bat
./setup_docker_postgres.bat

# Setup Python environment
cd hybrid_recommendation_movie
python3 -m venv .venv
source .venv/bin/activate
pip install discord.py torch pandas scikit-learn numpy psycopg2 python-dotenv
```

### Discord Bot Setup

#### 1. Create Discord Application
1. Go to [Discord Developer Portal](https://discord.com/developers/applications)
2. Click "New Application" and name it "Lupe" or "CineSync"
3. Navigate to "Bot" section and click "Add Bot"
4. Copy the bot token for configuration

#### 2. Configure Bot Token
```bash
# Copy the example environment file
cp .env.example .env

# Edit the .env file with your actual values
DISCORD_TOKEN=your_bot_token_here
DB_HOST=localhost
DB_NAME=cinesync
DB_USER=postgres
DB_PASSWORD=your_database_password_here
DB_PORT=5432
```

#### 3. Invite Bot to Server
Use this URL (replace YOUR_CLIENT_ID):
```
https://discord.com/api/oauth2/authorize?client_id=YOUR_CLIENT_ID&permissions=379968&scope=bot
```

Required permissions:
- Send Messages
- Embed Links
- Read Message History
- Use Slash Commands

## ğŸ” Usage Examples

### Discord Bot Commands

#### Lupe AI Recommendations
```
/recommend mixed              # Get 5 mixed movie + TV recommendations
/recommend mixed 10           # Get 10 mixed recommendations
/recommend movie 5            # Get 5 movie-only recommendations
/recommend tv 8               # Get 8 TV show-only recommendations
```

Response:
```
ğŸ¤– Lupe AI Recommendations (Mixed)

ğŸ¬ The Shawshank Redemption
ğŸ“Š Score: 94.2% | ğŸ­ Drama | Type: Movie

ğŸ“º Breaking Bad
ğŸ“Š Score: 92.1% | ğŸ­ Drama, Crime | Type: TV Show

ğŸ¬ Pulp Fiction  
ğŸ“Š Score: 91.8% | ğŸ­ Crime, Drama | Type: Movie

ğŸ“º The Wire
ğŸ“Š Score: 89.3% | ğŸ­ Crime, Drama | Type: TV Show
```

#### Cross-Content Recommendations
```
/cross_recommend tv movie         # Movies based on your TV preferences
/cross_recommend movie tv         # TV shows based on your movie preferences
```

#### Similar Content Search
```
/similar "The Matrix"           # Movies like The Matrix
/similar "Breaking Bad"         # Shows like Breaking Bad
/similar "Inception"            # Similar content (movies or shows)
/similar "Game of Thrones"      # Similar to Game of Thrones
```

#### Content Rating and Management
```
/rate "The Dark Knight" 5       # Rate a movie 5 stars
/rate "Breaking Bad" 5          # Rate a TV show 5 stars
/my_ratings                     # View all your content ratings
/lupe_status                    # View Lupe AI model information
```

#### User Profile Management
```
/profile                                 # View your profile
/stats                                   # View bot statistics
/help                                    # Show all commands
```

## ğŸ“Š AI Model Architecture

### Neural Network Design

CineSync uses a hybrid neural network architecture combining collaborative filtering and content-based recommendations:

```python
class HybridRecommenderModel(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim=64, hidden_dim=128):
        super().__init__()
        
        # Embedding layers for collaborative filtering
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)
        
        # Neural network layers
        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc3 = nn.Linear(hidden_dim // 2, 1)
        
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, user_ids, item_ids):
        # Get embeddings
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        
        # Concatenate and predict
        x = torch.cat([user_emb, item_emb], dim=1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        return torch.sigmoid(self.fc3(x)) * 5.0  # Scale to 0-5 rating range
```

### Training Process

#### Data Sources for Movies
- **MovieLens Dataset**: 32 million ratings from 280,000 users on 58,000 movies
- **TMDB Integration**: Comprehensive movie metadata, genres, cast, and crew
- **Netflix Movies Dataset**: Available in `/movies/netflix_movies.csv` - contains 6,131 Netflix movie entries. Source: [Kaggle Netflix Movies and TV Shows Dataset]

#### Data Sources for TV Shows
- **Netflix TV Shows Dataset**: Available in `/tv/netflix_tv_shows.csv` - contains 2,676 Netflix TV show entries. Source: [Kaggle Netflix Movies and TV Shows Dataset]
- **Full TMDb TV Shows Dataset**: 150,000+ TV shows with comprehensive metadata 
- **MyAnimeList Dataset**: 300,000 users, 80 million ratings for series and anime 
- **IMDb TV Series Dataset**: Industry-standard TV series ratings and metadata 

#### [Dataset Link]https://www.kaggle.com/datasets/nott1m/lupe-ai-training-dataset

#### Training Configuration
```python
# Training parameters optimized for hybrid model
BATCH_SIZE = 64          # Optimized for GPU memory
EMBEDDING_SIZE = 64      # Rich representations
EPOCHS = 20             # Sufficient for convergence
LEARNING_RATE = 0.001   # Adam optimizer
DROPOUT = 0.2           # Regularization
```

## ğŸ–¥ï¸ System Performance

### Benchmark Results

#### Model Performance
```
Dataset Size: Movies + TV Shows combined training
Training Time: ~1 hour (20 epochs on GPU)
GPU Memory: ~4GB peak usage
Validation Accuracy: 85%+ for personalized recommendations
Response Time: <100ms for recommendation generation
```

#### Discord Bot Performance
```
Command Response Time: 200-500ms average
Concurrent Users: 50+ per bot instance
Memory Usage: ~100MB per bot instance
Database Queries: <50ms average response time
Uptime: 99%+ availability target
```

## ğŸ“± Discord Bot Features

### Command System

#### Core Commands
- `/recommend [content_type] [count]` - Get personalized movie/TV recommendations (mixed, movie, tv)
- `/cross_recommend <source> <target>` - Cross-content recommendations (movieâ†’tv or tvâ†’movie)
- `/similar <content_title>` - Find similar movies or shows
- `/rate <content_title> <rating>` - Rate movies and TV shows (1-5 stars)
- `/my_ratings` - View all your content ratings
- `/lupe_status` - Check Lupe AI model status and capabilities
- `/stats` - System statistics and health
- `/help` - Command documentation

#### Advanced Features
- **Mixed Content**: Seamlessly blend movie and TV show recommendations
- **Cross-Content Intelligence**: Get TV shows based on movie preferences and vice versa
- **Rich Embeds**: Beautiful content displays with ratings, genres, and content type indicators
- **Universal Rating**: Rate both movies and TV shows to improve all recommendations
- **Smart Content Detection**: Automatically identifies content type when searching
- **Async Processing**: Non-blocking commands for responsive interactions

#### User Experience
- **Content Type Indicators**: Clear ğŸ¬ movie and ğŸ“º TV show emojis
- **Unified Interface**: Same commands work for both movies and TV shows
- **Smart Recommendations**: AI learns from your ratings across all content types
- **Cross-Pollination**: Movie ratings influence TV recommendations and vice versa
- **Content Discovery**: Find new content through similarity and cross-content features

### Bot Configuration

#### Environment Variables
```bash
DISCORD_TOKEN=your_bot_token_here        # Required: Discord bot token
DB_HOST=localhost                        # Database host
DB_NAME=cinesync                         # Database name
DB_USER=postgres                         # Database user
DB_PASSWORD=your_database_password_here  # Database password
DB_PORT=5432                            # Database port
DEBUG=true                              # Enable debug logging
```

## ğŸ” Data Sources and Training

### Current Datasets (Movies)

#### MovieLens Dataset
- **Size**: 32 million ratings, 58,000 movies, 280,000 users
- **Rating Scale**: 0.5 to 5.0 stars in 0.5 increments
- **Quality**: High-quality, research-grade dataset
- **Usage**: Primary training data for collaborative filtering

#### TMDB (The Movie Database)
- **Movies**: Comprehensive movie metadata including genres, cast, crew
- **Images**: Movie posters, backdrops, and promotional images
- **Ratings**: User ratings and critical scores
- **Usage**: Content-based features and movie information display

### Planned Datasets (TV Shows)

#### 1. Full TMDb TV Shows Dataset (150K Shows)
- **Source**: Kaggle - TMDB API
- **Size**: 150,000+ TV shows
- **Features**: Comprehensive metadata including genres, ratings, cast, crew, episode data, networks
- **Quality**: High - regularly updated, clean data structure
- **Usage**: Primary TV show metadata and content-based filtering

#### 2. MyAnimeList Dataset (300K Users, 80M Ratings)
- **Source**: Kaggle - MyAnimeList.net
- **Size**: 300,000 users, 14,000 anime/series, 80 million ratings
- **Features**: User profiles, detailed ratings, reviews, watching status, episode progress
- **Quality**: Excellent for collaborative filtering - real user behavior data
- **Usage**: TV series collaborative filtering and user preference modeling

#### 3. IMDb TV Series Dataset
- **Source**: Kaggle - IMDb data
- **Size**: Comprehensive TV series collection
- **Features**: Series metadata, episode information, ratings, cast, genres, years
- **Quality**: High reliability, industry standard
- **Usage**: Content-based filtering and quality validation

### Data Processing Pipeline

#### Data Cleaning and Preparation
```python
def process_content_data(content_df, min_ratings=10):
    # Remove content with too few ratings
    content_counts = content_df['content_id'].value_counts()
    valid_content = content_counts[content_counts >= min_ratings].index
    content_df = content_df[content_df['content_id'].isin(valid_content)]
    
    # Normalize ratings to 0-1 scale
    scaler = MinMaxScaler()
    content_df['rating_scaled'] = scaler.fit_transform(content_df[['rating']])
    
    return content_df, scaler
```

#### Feature Engineering
- **Genre Encoding**: Multi-hot encoding for content genres
- **Content Type**: Movie vs TV show classification features
- **Temporal Features**: Release year, rating timestamp analysis
- **User Profiles**: Cross-content preference modeling
- **Interaction Features**: User-genre preferences, content type preferences

## ğŸ”’ Security and Privacy

### Data Protection
- **Encryption**: All user data encrypted at rest and in transit
- **Access Control**: Database access restricted to application
- **Data Minimization**: Only collect necessary data for recommendations
- **Privacy by Design**: No personally identifiable information stored
- **GDPR Compliance**: User data rights and deletion capabilities

### Database Security
- **Authentication**: Secure database authentication
- **Network Security**: Database accessible only from application
- **Input Validation**: Comprehensive validation of all input parameters
- **SQL Injection Prevention**: Parameterized queries and ORM usage
- **Monitoring**: Database access logging and monitoring

## ğŸŒ Deployment Options

### Docker Deployment (Recommended)

```bash
# Start PostgreSQL database
docker-compose up -d postgres

# Verify database is ready
docker-compose exec postgres pg_isready -U postgres -d cinesync

# Start Discord bot
cd hybrid_recommendation_movie
python main.py
```

### Database Schema
The system automatically creates these tables:
- **feedback**: User feedback and ratings
- **user_ratings**: Discord user rating history
- **movies**: Movie metadata and information
- **ratings**: Training ratings data

### Production Considerations

#### Monitoring
- **Health Checks**: Database connectivity monitoring
- **Logging**: Structured logging with rotation
- **Metrics**: User engagement and system performance metrics
- **Error Tracking**: Comprehensive error logging and reporting

#### High Availability
- **Database Backup**: Regular PostgreSQL backups
- **Failover**: Automatic restart capabilities
- **Resource Monitoring**: Memory and CPU usage tracking
- **Scaling**: Horizontal scaling support for multiple bot instances

## ğŸ›£ï¸ Roadmap

### Short Term (Next 3 months)
- **TV Show Integration**: Complete TV show recommendation system
- **Enhanced Feedback**: More sophisticated user feedback collection
- **Cross-Content Learning**: Learn TV preferences from movie ratings
- **Performance Optimization**: Improved model inference speed
- **Additional Commands**: More Discord bot features and commands

### Medium Term (3-6 months)
- **Web Interface**: Browser-based recommendation interface
- **Mobile App**: Native mobile applications for iOS and Android
- **Social Features**: Friend recommendations and social discovery
- **Streaming Integration**: Integration with Netflix, Hulu, etc.
- **Multi-language Support**: Recommendations in multiple languages

### Long Term (6+ months)
- **Real-time Learning**: Live model updates based on user feedback
- **Computer Vision**: Content poster and scene analysis
- **Voice Integration**: Voice-activated recommendations
- **Enterprise Features**: Advanced features for commercial deployment
- **API Monetization**: Premium API access for developers

## ğŸ¤ Contributing

### Development Setup
```bash
# Fork the repository
git clone https://github.com/yourusername/cine-sync-v2
cd cine-sync-v2

# Setup development environment
setup_docker_postgres.bat
cd hybrid_recommendation_movie
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Start development bot
python main.py
```

### Contribution Guidelines
- **Code Style**: Follow Python PEP 8 style guidelines
- **Testing**: Include tests for new features
- **Documentation**: Update documentation for changes
- **Database**: Ensure database migrations are included
- **Compatibility**: Maintain backward compatibility when possible

## ğŸ“ Support and Community

### Community Channels
- **GitHub Issues**: Report bugs and request features
- **Discussions**: GitHub Discussions for community support
- **Documentation**: Comprehensive setup and usage guides

### Support Tiers

#### Community Support (Free)
- GitHub issues and discussions
- Documentation and setup guides
- Basic troubleshooting assistance
- Community-driven support

#### Professional Support (Custom)
- Priority email support
- Custom deployment assistance
- Performance optimization guidance
- Integration support for enterprise systems

## ğŸ† Benchmarks and Performance

### Recommendation Quality Metrics

#### Current Performance (Movies Only)
```
Model Performance:
- RMSE: 0.147 (lower is better)
- MAE: 0.112 (lower is better)
- Hit Rate@10: 83.2% (higher is better)
- Coverage: 94.7% (higher is better)
```

#### Planned Performance (Movies + TV Shows)
```
Expected Combined Performance:
- Cross-Content Accuracy: 80%+ 
- Cold Start Performance: 70%+ for new users
- Response Time: <200ms for recommendations
- Database Query Time: <50ms average
```

### System Performance Benchmarks

#### Discord Bot Performance
```
Bot Performance:
- Memory Usage: 100MB average
- CPU Usage: 5-10% under normal load
- Commands per Second: 20+ concurrent
- Response Time: 300ms average (including database)
- Uptime: 99%+ availability target
```

#### Database Performance
```
PostgreSQL Performance:
- Query Response Time: <50ms average
- Concurrent Connections: 100+
- Storage Requirements: <5GB for user data
- Backup Time: <10 minutes daily
```

## ğŸ¯ Use Cases and Applications

### Personal Entertainment
- **Movie Night Planning**: Get recommendations for group viewing
- **Binge-Watching**: Discover new TV series to binge
- **Cross-Content Discovery**: Find movies based on loved TV shows
- **Genre Exploration**: Expand your taste across different content types
- **Nostalgic Viewing**: Rediscover classics in both movies and TV

### Social and Community
- **Discord Server Entertainment**: Shared recommendations for communities
- **Viewing Parties**: Curated selections for group watching
- **Family Entertainment**: Age-appropriate recommendations for family time
- **Friend Groups**: Consensus recommendations across different content types
- **Discussion Groups**: Content recommendations for review and discussion

### Educational and Research
- **Media Studies**: Explore relationships between movies and TV shows
- **Cultural Learning**: Discover international content across formats
- **Genre Analysis**: Deep dive into genre conventions across media types
- **Academic Research**: Dataset and system for recommendation research
- **Content Creation**: Inspiration for reviews and analysis content

## ğŸ”§ Advanced Configuration

### Model Configuration

#### Training Parameters
```python
# config/model_config.py
MODEL_CONFIG = {
    'embedding_dim': 64,
    'hidden_dim': 128,
    'dropout_rate': 0.2,
    'learning_rate': 0.001,
    'batch_size': 64,
    'epochs': 20,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}
```

#### Database Configuration
```python
# config/database_config.py
DATABASE_CONFIG = {
    'host': 'localhost',
    'database': 'cinesync',
    'user': 'postgres',
    'password': 'your_database_password_here',
    'port': 5432,
    'pool_size': 10,
    'max_connections': 20
}
```

### Discord Bot Configuration

#### Bot Settings
```python
# config/bot_config.py
BOT_CONFIG = {
    'command_prefix': '!',
    'max_recommendations': 15,
    'default_recommendations': 5,
    'cache_timeout': 3600,  # 1 hour
    'enable_feedback': True,
    'enable_profiles': True,
    'debug_mode': True
}
```

## ğŸ“‹ Troubleshooting Guide

### Common Issues

#### Database Connection Issues
```bash
# Issue: Cannot connect to PostgreSQL
Error: connection to server at "localhost", port 5432 failed

# Solutions:
1. Ensure Docker is running
2. Start PostgreSQL container: docker-compose up -d postgres
3. Check port availability: netstat -an | grep 5432
4. Verify .env file configuration
```

#### Discord Bot Issues
```bash
# Issue: Bot doesn't respond to commands
Error: 401 Unauthorized

# Solutions:
1. Verify Discord token in .env file
2. Check bot permissions in Discord server
3. Ensure bot is invited with correct permissions
4. Check Python environment and dependencies
```

#### Model Training Issues
```bash
# Issue: CUDA out of memory during training
Error: RuntimeError: CUDA out of memory

# Solutions:
1. Reduce batch size in model configuration
2. Use CPU training: device='cpu'
3. Enable gradient checkpointing
4. Close other GPU applications
```

### Diagnostic Commands

#### System Health Check
```bash
# Check Docker containers
docker-compose ps

# Check database connectivity
docker-compose exec postgres pg_isready -U postgres -d cinesync

# Check Python dependencies
pip list | grep -E "(torch|discord|pandas|psycopg2)"

# Test bot locally
python main.py --test-mode
```

## ğŸ“ License and Legal

### License Information

This project is licensed under the MIT License for all core components:
- **CineSync AI Model**: MIT License
- **Lupe Discord Bot**: MIT License
- **Documentation**: Creative Commons Attribution 4.0

### Data Sources Attribution

```
MovieLens Dataset:
F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context.

TMDB Data:
This product uses the TMDB API but is not endorsed or certified by TMDB.

Planned TV Show Datasets:
- TMDb TV Shows Dataset: Licensed under TMDB API Terms
- MyAnimeList Dataset: Research and educational use
- IMDb TV Series Data: Non-commercial research use
```

---

## ğŸ¬ Getting Started

Ready to start discovering amazing movies and TV shows with CineSync? Here's your quick start checklist:

### âœ… Quick Setup Checklist

1. **Prerequisites**: Install Python, Docker, and Git
2. **Clone**: `git clone https://github.com/yourusername/cine-sync-v2`
3. **Database**: Run `setup_docker_postgres.bat`
4. **Environment**: Setup Python virtual environment in `lupe(python)`
5. **Dependencies**: `pip install discord.py torch pandas scikit-learn numpy psycopg2 python-dotenv`
6. **Discord Token**: Set your Discord token in `.env`
7. **Test**: Use `/recommend` in your Discord server

### ğŸ¯ First Commands to Try

```
/help                          # See all available commands
/recommend mixed 10            # Get mixed movie + TV recommendations
/recommend tv 5                # Get TV show recommendations
/cross_recommend movie tv      # Get TV shows based on movie preferences
/similar "Breaking Bad"        # Find similar content
/rate "The Office" 5           # Rate movies and TV shows
/my_ratings                    # View all your ratings
/lupe_status                   # Check Lupe AI status
```

### ğŸš€ What's Next?

- **Explore Mixed Content**: Try `/recommend mixed` for both movies and TV shows
- **Cross-Content Discovery**: Use `/cross_recommend` to find TV shows based on movie preferences
- **Rate Everything**: Rate both movies and TV shows to improve recommendations
- **Similar Content**: Use `/similar` to find content like your favorites
- **Share with Friends**: Invite Lupe to your Discord servers
- **Check Status**: Use `/lupe_status` to see what models are loaded

---

**ğŸ¬ Welcome to CineSync v2 - Your Personal Entertainment AI! ğŸ¿ğŸ“º**

*"Great movies and shows are just a command away"*

---

Copyright Â© 2025 CineSync Movie & TV Recommendation System. MIT License.
